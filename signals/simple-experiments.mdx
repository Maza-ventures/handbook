---
title: "Simple Experiments"
description: "Testing assumptions without elaborate frameworks"
---

Instead of trying to predict what will work through analysis, just test it. Simple experiments give you real data about your specific situation faster and cheaper than any amount of market research.

## Why Experiments Beat Analysis

**Experiments test your specific assumptions about your specific business.** Market research tests general assumptions about general markets.

**Experiments give you evidence, not opinions.** People lie in surveys and interviews, sometimes without realizing it. But behavior in experiments reveals what people actually value.

**Experiments are cheap and fast.** You can test most startup assumptions in a week with almost no budget. Analysis takes longer and usually costs more.

**Experiments compound.** Each experiment teaches you something that improves the next experiment. Analysis doesn't get better by doing more of it.

## The Basic Experimental Framework

**Start with a specific assumption.** Not "people want our product" but "small restaurants in Amman will pay $50/month for inventory management software."

**Design the simplest possible test.** What's the smallest thing you can build or do to test whether this assumption is true?

**Set clear success criteria before you start.** How many restaurants need to say yes for you to consider the assumption validated? How many need to pay you? Define this upfront.

**Run the experiment for a specific time period.** One week, two weeks, one month. Don't run experiments indefinitely hoping they'll eventually work.

## Demand Testing Experiments

**Landing page + ads:** Build a simple page describing your product and run ads to drive traffic. Measure how many people give you their email or try to buy something that doesn't exist yet.

**Manual service delivery:** Before building software, do the work manually for a few customers. This tests whether people value the outcome, not just whether they like your interface.

**Fake door testing:** Add a button or menu item for a feature you haven't built yet. See how many people click it. This tests whether people want the feature before you build it.

**Price testing:** Tell potential customers your price and see how they react. Not in a survey, but in a real conversation where they have to decide whether to pay you.

## Product-Market Fit Experiments

**Usage retention:** Give your product to 10 people. How many are still using it after two weeks? After a month? If people stop using your product quickly, you don't have product-market fit yet.

**Referral testing:** Ask your best users to recommend your product to someone they know. How many are willing to do this? How many of their referrals actually sign up and use the product?

**Payment willingness:** Stop providing your product for free and start charging for it. How many current users convert to paying customers? How many new users are willing to pay from the beginning?

**Competition displacement:** Are people switching from existing solutions to use your product? Or are you just getting people who weren't solving this problem before?

## Channel Testing Experiments

**Direct outreach:** Email or call potential customers directly. What response rate do you get? How many convert to actual users?

**Partner distribution:** Find businesses that already serve your target customers. Can they successfully sell your product to their customers?

**Content marketing:** Write about problems your customers have. Does this attract people who actually become users?

**Community presence:** Participate in online communities where your customers hang out. Can you get people interested in your product without being salesy?

Test each channel independently so you know which ones actually work for your business.

## Pricing Experiments

**Price sensitivity testing:** Offer the same product at different prices to different customer segments. How much can you charge before people stop buying?

**Payment method testing:** Some MENA customers prefer annual contracts, others prefer monthly payments, others want to pay by invoice. Test what works for your specific market.

**Value metric testing:** Charge per user, per transaction, per month, or per outcome. Which pricing model feels fair to customers and generates the most revenue for you?

**Discount sensitivity:** How much do discounts actually affect purchase decisions? Sometimes higher prices work better than lower prices.

## Common Experimental Mistakes

**Testing too many things at once.** If you change your price AND your messaging AND your target customer, you won't know which change caused different results.

**Running experiments too long.** Most startup experiments should give you directional answers within 1-2 weeks. If you need months to see results, the signal probably isn't strong enough to matter.

**Ignoring negative results.** If an experiment fails, that's valuable information. Don't keep running variations hoping to get the answer you want.

**Not talking to people during experiments.** Numbers tell you what happened, but conversations tell you why it happened. Do both.

## MENA-Specific Experimental Considerations

**Family and social network decision-making:** Test whether the person using your product is the same person who decides to pay for it. In many MENA contexts, these are different people.

**Language and cultural context:** Test your product with users who primarily think in Arabic, Turkish, or French, even if they speak English professionally. Cultural context affects how people perceive value.

**Payment method preferences:** Test different payment methods early. Credit cards, bank transfers, mobile payments, and cash on delivery work differently across MENA markets.

**Regional variation:** What works in Dubai might not work in Cairo. Test locally before assuming regional applicability.

## When Experiments Are Wrong

Experiments can give you false negatives (showing something doesn't work when it actually would work with more time or better execution) and false positives (showing something works in a test environment but fails at scale).

The solution isn't to stop experimentingâ€”it's to run better experiments and interpret results carefully.

Small experiments are particularly bad at predicting network effects, viral growth, and long-term behavior changes. For these, you might need longer experiments or different experimental approaches.

## The Experimental Mindset

Treat everything as a hypothesis to be tested rather than a truth to be defended. Your assumptions about your market, your product, your pricing, and your distribution strategy might all be wrong.

Most founders are wrong about most things when they start. The winners are the founders who figure out what they're wrong about quickly and cheaply.

In MENA markets, where conventional startup wisdom often doesn't apply, experimental thinking is especially valuable. Don't assume what works elsewhere will work here. Test everything.